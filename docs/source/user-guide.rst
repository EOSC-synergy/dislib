User guide
==========

This guide covers the basics of using dislib, and details on the different
algorithms included in the library.

dislib provides two major programming interfaces: an API to manage data in a
distributed way, and an estimator-based interface to work with different
machine learning models. The distributed data API is built around the
concept of distributed arrays (ds-arrays), and the estimator-based interface
has been inspired by `scikit-learn <https://scikit-learn.org>`_.

The term estimator-based interface means that all the machine learning
models in dislib are provided as `estimator <https://scikit-learn
.org/stable/glossary.html#term-estimators>`_ objects. An estimator is
anything that learns from data given certain parameters. dislib estimators
implement the same API as scikit-learn, which is mainly based in the *fit*
and *predict* operators.

The typical workflow in dislib consists of the following steps:

 1. Reading input data into a ds-array
 2. Creating an estimator object
 3. Fitting the estimator with the input data
 4. Getting information from the model's estimator or applying the model to
    new data

An example of performing :class:`K-means <dislib.cluster.kmeans.base.KMeans>`
clustering with dislib is as follows:

.. code:: python

    import dislib as ds
    from dislib.cluster import KMeans

    # load data into a ds-array
    x = ds.load_txt_file("/path/to/file", block_size=(100, 100))

    # create estimator object
    kmeans = KMeans(n_clusters=10)

    # fit estimator
    kmeans.fit(x)

    # get information from the model
    cluster_centers = kmeans.centers

It is worth noting that, although the code above looks completely sequential,
all dislib algorithms and operations are paralellized using `PyCOMPSs
<https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/>`_.

How to run dislib
-----------------

dislib can be installed and used as a regular Python library. However,
dislib makes use of PyCOMPSs directives internally to parallelize all the
computation. This means that applications using dislib need to be executed
with PyCOMPSs. This can be done with the ``runcompss`` or
``enqueue_compss`` commands:

.. code:: bash

    runcompss my_dislib_application.py

For more information on how to start running your dislib applications, refer
to the :doc:`quickstart guide <quickstart>`.

Distributed arrays
------------------

Distributed arrays (ds-arrays) are the main data structure used in dislib.
In essence, a ds-array is a matrix divided in blocks that are typically
stored remotely. Each block of a ds-array is a `NumPy <https://numpy.org/>`_
array or a SciPy `CSR matrix <https://docs.scipy
.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse
.csr_matrix>`_, depending on the kind of data used to create the ds-array.
dislib provides an API similar to NumPy to work with ds-arrays in a
completely sequential way. However, ds-arrays are not stored in the local
memory. This means that ds-arrays can store much more data than regular
NumPy arrays.

|

.. image:: ./_static/img/ds-array.png
    :align: center
    :width: 400px

|

All operations on ds-arrays are internally parallelized with PyCOMPSs. The
degree of parallelization is controlled using the array's block size. Block
size defines the number of rows and columns of each block in a ds-array.
Sometimes a ds-array cannot be completely split into uniform blocks of a
given size. In these cases, some blocks of the ds-array will be slightly
smaller than the defined block size. Choosing the right block size is essential
to be able to exploit dislib's full potential.

Choosing the right block size
.............................

The ideal block size depends on the available resources and the
application. The number of tasks generated by a dislib application is
inversely proportional to the block size. This means that small blocks allow
for higher parallelism as the computation is divided in more tasks. However,
handling a large number of blocks also produces overhead that can have a
negative impact on performance. Thus, the optimal block size will allow the
full utilization of the available resources without adding too much overhead.

In addition to this, block size also affects the amount of data that tasks load
into memory. This means that block size should never be bigger than the
amount of available memory per processor.

Most estimators in dislib process ds-arrays in blocks of rows (or samples).
This means that the optimal block size when using these estimators might be
to have as many *rectangular* blocks as available processors. For example,
in a computer with 4 processors, K-means (and other similar estimators)
will usually fit a 100x100 ds-array faster using blocks of size 25x100 than
using blocks of size 50x50, even though the number of blocks is 4 in
both cases.

The diagram below shows how the K-means estimator would process an 8x8
ds-array split in different block sizes.

|

.. image:: ./_static/img/ds-array-access.png
    :align: center
    :width: 700px

|

This is because K-means loads a full block of rows on each task.
Using 4x4 blocks only generates 2 tasks, while using 2x8 blocks generates 4
tasks and provides more parallelism in a system with 4 processors. Using 2x4
blocks provides the same parallelism as 2x8 blocks, but has the overhead of
dealing with five additional blocks. If we were only doing K-means
clustering, 2x8 blocks would probably be the optimal choice in this case.

However, some estimators like
:class:`ALS <dislib.recommendation.als.base.ALS>` benefit from having a uniform
number of blocks both vertically and horizontally. In these cases, it might
be better to split the ds-array in NxN blocks, where N is the number of
processors. This splitting strategy can be a good choice if you are not sure
on which block size to use in your application.

Below you will find more details on the parallelization strategy and data
access pattern of each estimator. This can help you to define the
appropriate block size in your application.

Finally, keep in mind that in the example above, each K-means task loads a
full block of rows into memory. This means that, when using K-means, you
should choose a block size that ensures that a block of rows is not larger
than the available memory per processor in your platform.

Creating arrays
...............

dislib provides a set of routines to create ds-arrays from scratch,
or using existing data. The :ref:`API reference <array-creation>` contains the
full list of available routines. For example,
:func:`random_array <dislib.random_array>` can be used to create
a ds-array with random data:

.. code:: python

    import dislib as ds

    x = ds.random_array(shape=(100, 100), block_size=(20, 20))

Another way of creating a ds-array is by reading data from a file. dislib
supports common data formats, such as CSV and `SVMLight <http://svmlight
.joachims.org/>`_, using
:func:`load_txt_file <dislib.load_txt_file>` and
:func:`load_svmlight_file <dislib.load_svmlight_file>`.

Slicing
.......

Similar to NumPy arrays, ds-arrays provide different types of slicing. The
result of an slicing operation is a new ds-array with a subset of elements
of the original ds-array.

Currently, these are the supported slicing methods:

``x[i]``
  returns the ith row of x.

``x[i,j]``
  returns the element at the (i,j) position.

``x[i:j]``
  returns a set of rows (from i to j), where i and j are optional.

``x[:, i:j]``
  returns a set of columns (from i to j), where i and j are optional.

``x[[i,j,k]]``
  returns a set of non-consecutive rows.

``x[:, [i,j,k]]``
  returns a set of non-consecutive columns.

``x[i:j, k:m]``
  returns a set of elements, where i, j, m, and n are optional.

Other operations
................

Classification
--------------

Cascade SVM
...........

Random forest classifier
........................

Clustering
----------

K-means
.......

DBSCAN
......

Gaussian mixtures
.................

Regression
----------

Linear regression
.................

Decomposition
-------------

Principal component analysis
............................

Pre-processing
--------------

Standard scaler
...............

Neighbors
---------

K-nearest neighbors
...................

Model selection
---------------

Grid search
...........
